{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.Word Analysis in NLP"
      ],
      "metadata": {
        "id": "rf9YSMP_2sD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XstTVKSxEBpe",
        "outputId": "5c0b78f2-9951-44db-a6d1-a0f6bb504a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter:package\n",
            "\n",
            "Analysis of 'package':\n",
            "\n",
            "- Definition: a collection of things wrapped or boxed together\n",
            "- Part of Speech: n\n",
            "\n",
            "- Definition: a wrapped container\n",
            "- Part of Speech: n\n",
            "\n",
            "- Definition: (computer science) written programs or procedures or rules and associated documentation pertaining to the operation of a computer system and that are stored in read/write memory\n",
            "- Part of Speech: n\n",
            "- Example: the market for software is expected to expand\n",
            "- Synonyms: package, software package, computer software, software system, software, software program\n",
            "- Antonyms: hardware\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "def analyze_word(word):\n",
        "  synsets = wordnet.synsets(word)\n",
        "  if not synsets:\n",
        "    return \"No analysis found.\"\n",
        "  result = f\"\\nAnalysis of '{word}':\\n\"\n",
        "  for syn in synsets[:3]:\n",
        "    synonyms = {lemma.name().replace('_', ' ') for lemma in syn.lemmas()}\n",
        "    antonyms = {lemma.antonyms()[0].name().replace('_', ' ') for lemma in syn.lemmas() if\n",
        "    lemma.antonyms()}\n",
        "    result += f\"\\n- Definition: {syn.definition()}\\n\"\n",
        "    result += f\"- Part of Speech: {syn.pos()}\\n\"\n",
        "    if syn.examples():\n",
        "      result += f\"- Example: {syn.examples()[0]}\\n\"\n",
        "      result += f\"- Synonyms: {', '.join(synonyms) or 'None'}\\n\"\n",
        "      result += f\"- Antonyms: {', '.join(antonyms) or 'None'}\\n\"\n",
        "      result += \"-\" * 30\n",
        "  return result\n",
        "print(analyze_word(input(\"Enter:\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Word Generation in NLP"
      ],
      "metadata": {
        "id": "6sapv6Pb3EgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "def get_related_words(word):\n",
        "  synsets = wn.synsets(word)\n",
        "  related_words = set()\n",
        "  for syn in synsets:\n",
        "    for lemma in syn.lemmas():\n",
        "      related_words.add(lemma.name().replace(\"_\", \" \"))\n",
        "    for hyper in syn.hypernyms():\n",
        "      for lemma in hyper.lemmas():\n",
        "        related_words.add(lemma.name().replace(\"_\", \" \"))\n",
        "    for hypo in syn.hyponyms():\n",
        "      for lemma in hypo.lemmas():\n",
        "        related_words.add(lemma.name().replace(\"_\", \" \"))\n",
        "  return list(related_words)[:10]\n",
        "print(\" WordNet-based Word Explorer\")\n",
        "word = input(\"Enter a word: \").lower().strip()\n",
        "related = get_related_words(word)\n",
        "if related:\n",
        "  print(\"\\n Related Words:\", \", \".join(related))\n",
        "else:\n",
        "  print(\"\\n No related words found in WordNet.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au5XLbfBVSnU",
        "outputId": "11e2b46b-3cac-4cec-b7e7-d3ac766aee2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " WordNet-based Word Explorer\n",
            "Enter a word: glad\n",
            "\n",
            " Related Words: iridaceous plant, gladiolus, beaming, sword lily, gladiola, glad, happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Morphological Analysis in NLP"
      ],
      "metadata": {
        "id": "6_xHRmSs3Jt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "def morphology_analysis(word):\n",
        "# Create instances for stemmer and lemmatizer\n",
        "  ps = PorterStemmer()\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  # Perform stemming and lemmatization\n",
        "  stemmed_word = ps.stem(word)\n",
        "  lemmatized_word = lemmatizer.lemmatize(word)\n",
        "  # Display the results\n",
        "  print(\"\\nOriginal Word:\", word)\n",
        "  print(\"Stemmed Word:\", stemmed_word)\n",
        "  print(\"Lemmatized Word:\", lemmatized_word)\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "  word = input(\"Enter a word for morphological analysis: \")\n",
        "  morphology_analysis(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObZfoOZbWYhY",
        "outputId": "6a7d249e-c127-4ea8-92ae-d927f14eaaf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word for morphological analysis: running\n",
            "\n",
            "Original Word: running\n",
            "Stemmed Word: run\n",
            "Lemmatized Word: running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.N-Gram Analysis in NLP"
      ],
      "metadata": {
        "id": "gyeUNL1T3QMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "def preprocess(text):\n",
        "  \"\"\"Convert text to lowercase and remove punctuation.\"\"\"\n",
        "  return re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "def generate_ngrams(text, n):\n",
        "  \"\"\"Generate n-grams from text.\"\"\"\n",
        "  words = text.split()\n",
        "  return [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
        "def ngram_model(text, n):\n",
        "  \"\"\"Create an n-gram model with frequency counts.\"\"\"\n",
        "  ngrams = generate_ngrams(preprocess(text), n)\n",
        "  ngram_count = defaultdict(int)\n",
        "  for ngram in ngrams:\n",
        "\n",
        "    ngram_count[ngram] += 1\n",
        "  return ngram_count\n",
        "# ---- User Interaction ----\n",
        "print(\" N-gram Frequency Model\")\n",
        "user_text = input(\"Enter your text:\\n\")\n",
        "try:\n",
        "  n = int(input(\"\\nEnter the n-gram size (e.g., 1 for unigrams, 2 for bigrams): \"))\n",
        "  if n < 1:\n",
        "    raise ValueError\n",
        "except ValueError:\n",
        "  print(\"Invalid n-gram size. Please enter a positive integer.\")\n",
        "else:\n",
        "  model = ngram_model(user_text, n)\n",
        "  print(f\"\\n{n}-gram Frequencies:\")\n",
        "  for k, v in model.items():\n",
        "    print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "id": "7vg1sD47eTqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13b7ae21-319f-44c5-bce3-b5b38ca82347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " N-gram Frequency Model\n",
            "Enter your text:\n",
            "no boys are good and no girl\n",
            "\n",
            "Enter the n-gram size (e.g., 1 for unigrams, 2 for bigrams): 3\n",
            "\n",
            "3-gram Frequencies:\n",
            "('no', 'boys', 'are'): 1\n",
            "('boys', 'are', 'good'): 1\n",
            "('are', 'good', 'and'): 1\n",
            "('good', 'and', 'no'): 1\n",
            "('and', 'no', 'girl'): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.N-Gram Smoothing in NLP"
      ],
      "metadata": {
        "id": "KN0TNrb63XcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict, Counter\n",
        "# Download the punkt_tab resource\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt') #ensure punkt is also downloaded.\n",
        "def generate_ngrams(text, n):\n",
        "  words = nltk.word_tokenize(text.lower()) # Tokenization\n",
        "  ngram_list = list(ngrams(words, n)) # Generate n-grams\n",
        "  return ngram_list\n",
        "def compute_smoothed_probabilities(ngrams_list, ngram_counts, unigram_counts,vocab_size):\n",
        "  smoothed_probs = {}\n",
        "  for ngram in ngrams_list:\n",
        "    prefix = ngram[:-1] # Previous (n-1) words\n",
        "    word = ngram[-1] # Current word\n",
        "    count_ngram = ngram_counts[ngram]\n",
        "    count_prefix = unigram_counts[prefix] if prefix in unigram_counts else 0\n",
        "    # Apply Laplace Smoothing\n",
        "    smoothed_prob = (count_ngram + 1) / (count_prefix + vocab_size)\n",
        "    smoothed_probs[ngram] = smoothed_prob\n",
        "  return smoothed_probs\n",
        "# Main function\n",
        "\n",
        "def main():\n",
        "  text = input(\"Enter a sentence: \")\n",
        "  n = int(input(\"Enter the value of N for N-grams: \"))\n",
        "  # Generate n-grams\n",
        "  ngrams_list = generate_ngrams(text, n)\n",
        "  # Compute n-gram counts\n",
        "  ngram_counts = Counter(ngrams_list)\n",
        "  unigram_counts = Counter(generate_ngrams(text, n - 1)) if n > 1 else Counter(nltk.word_tokenize(text.lower()))\n",
        "  vocab_size = len(set(nltk.word_tokenize(text.lower()))) # Vocabulary size\n",
        "  # Compute smoothed probabilities\n",
        "  smoothed_probs = compute_smoothed_probabilities(ngrams_list, ngram_counts,\n",
        "  unigram_counts, vocab_size)\n",
        "  # Display results\n",
        "  print(\"\\nN-grams and their Smoothed Probabilities:\")\n",
        "  for ngram, prob in smoothed_probs.items():\n",
        "    print(f\"{ngram} -> {prob:.4f}\")\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "-Mx1B7jkeUAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d2e327-6618-462c-e0cd-0e2c2d21d787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: the dog chaes a cat\n",
            "Enter the value of N for N-grams: 2\n",
            "\n",
            "N-grams and their Smoothed Probabilities:\n",
            "('the', 'dog') -> 0.3333\n",
            "('dog', 'chaes') -> 0.3333\n",
            "('chaes', 'a') -> 0.3333\n",
            "('a', 'cat') -> 0.3333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.POS Tagging using Hidden Markov Model (HMM) in NLP"
      ],
      "metadata": {
        "id": "LrTM3m0U3dCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tag import hmm\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab resource\n",
        "# Load the Treebank corpus and split into training and test sets\n",
        "train_sents = treebank.tagged_sents()[:3000]\n",
        "test_sents = treebank.tagged_sents()[3000:]\n",
        "# Initialize and train the HMM tagger\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = trainer.train(train_sents)\n",
        "# Function to tag a new sentence\n",
        "\n",
        "def pos_tag_sentence(sentence):\n",
        "  tokens = word_tokenize(sentence)\n",
        "  tagged = tagger.tag(tokens)\n",
        "  return tagged\n",
        "# Get user input\n",
        "sentence = input(\"Enter a sentence to POS tag: \")\n",
        "# Tag the sentence and display the result\n",
        "tagged_sentence = pos_tag_sentence(sentence)\n",
        "print(\"\\nTagged Sentence:\")\n",
        "for word, tag in tagged_sentence:\n",
        "  print(f\"{word}: {tag}\")"
      ],
      "metadata": {
        "id": "rvHHx7wEgg0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5a7007-169c-4c27-b9df-b88d3cd910ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence to POS tag: she enjoys reading book\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/tag/hmm.py:333: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/tag/hmm.py:335: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/tag/hmm.py:331: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tagged Sentence:\n",
            "she: PRP\n",
            "enjoys: NNP\n",
            "reading: NNP\n",
            "book: NNP\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/tag/hmm.py:363: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.POS Tagging using Viterbi Decoding"
      ],
      "metadata": {
        "id": "knDaX38S4EIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# POS tags and probabilities\n",
        "states = ['Noun', 'Verb', 'Adjective']\n",
        "start_prob = {'Noun': 0.5, 'Verb': 0.3, 'Adjective': 0.2}\n",
        "transition_prob = {\n",
        "'Noun': {'Noun': 0.1, 'Verb': 0.6, 'Adjective': 0.3},\n",
        "'Verb': {'Noun': 0.4, 'Verb': 0.3, 'Adjective': 0.3},\n",
        "'Adjective': {'Noun': 0.5, 'Verb': 0.2, 'Adjective': 0.3}\n",
        "}\n",
        "emission_prob = {\n",
        "'Noun': {'dog': 0.4, 'cat': 0.4, 'runs': 0.1, 'fast': 0.1},\n",
        "'Verb': {'dog': 0.1, 'cat': 0.1, 'runs': 0.6, 'fast': 0.2},\n",
        "'Adjective': {'dog': 0.1, 'cat': 0.1, 'runs': 0.2, 'fast': 0.6}\n",
        "}\n",
        "\n",
        "def viterbi(sentence):\n",
        "  words = sentence.lower().split()\n",
        "  n, m = len(words), len(states)\n",
        "  viterbi_matrix = np.zeros((m, n))\n",
        "  backpointer = np.zeros((m, n), dtype=int)\n",
        "  for i, state in enumerate(states):\n",
        "    viterbi_matrix[i, 0] = start_prob[state] * emission_prob[state].get(words[0], 0.01)\n",
        "  for t in range(1, n):\n",
        "    for i, state in enumerate(states):\n",
        "      probs = [(viterbi_matrix[j, t-1] * transition_prob[prev][state] * emission_prob[state].get(words[t], 0.01), j)for j, prev in enumerate(states)]\n",
        "    viterbi_matrix[i, t], backpointer[i, t] = max(probs)\n",
        "  best_path = [np.argmax(viterbi_matrix[:, -1])]\n",
        "  for t in range(n-1, 0, -1):\n",
        "    best_path.insert(0, backpointer[best_path[0], t])\n",
        "  return list(zip(words, [states[i] for i in best_path]))\n",
        "# User input and tagging\n",
        "sentence = input(\"Enter a sentence: \")\n",
        "print(\"\\nPOS Tagged Sentence:\")\n",
        "for word, tag in viterbi(sentence):\n",
        "  print(f\"{word} ---> {tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdlGVvB34P2m",
        "outputId": "183b9488-02ed-429b-a97d-bdd62c38208f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: apple is a fruit\n",
            "\n",
            "POS Tagged Sentence:\n",
            "apple ---> Noun\n",
            "is ---> Adjective\n",
            "a ---> Adjective\n",
            "fruit ---> Adjective\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Building a POS Tagger in NLP"
      ],
      "metadata": {
        "id": "oJ1k4t0O4IE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pos_tagger_spacy(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    print(\"\\nPOS Tagging Results using SpaCy:\")\n",
        "    for token in doc:\n",
        "        print(f\"{token.text} ---> {token.pos_} ({token.tag_})\")\n",
        "\n",
        "sentence = input(\"Enter a sentence: \")\n",
        "pos_tagger_spacy(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga1LdfuZixdc",
        "outputId": "115436cc-a073-4f9d-dcc9-c47bc05f20c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: she is a good girl\n",
            "\n",
            "POS Tagging Results using SpaCy:\n",
            "she ---> PRON (PRP)\n",
            "is ---> AUX (VBZ)\n",
            "a ---> DET (DT)\n",
            "good ---> ADJ (JJ)\n",
            "girl ---> NOUN (NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Chunking in NLP"
      ],
      "metadata": {
        "id": "anQ4klJa5kzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Load SpaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def chunking_tree_spacy(sentence):\n",
        "    # Process the sentence using SpaCy\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    print(\"\\nNoun Phrase Chunks (SpaCy):\")\n",
        "    for chunk in doc.noun_chunks:\n",
        "        print(chunk.text)\n",
        "\n",
        "    # Displaying tree visualization using displacy\n",
        "    print(\"\\nDependency Tree Visualization:\")\n",
        "    displacy.render(doc, style=\"dep\", jupyter=True, options={\"compact\": True, \"distance\": 100})\n",
        "\n",
        "sentence = input(\"Enter a sentence: \")\n",
        "chunking_tree_spacy(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "7btwq6UUh_Ne",
        "outputId": "7e28bcfb-8f6c-452f-860d-fea6bf1cd55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: she is a bad idea girl\n",
            "\n",
            "Noun Phrase Chunks (SpaCy):\n",
            "she\n",
            "a bad idea girl\n",
            "\n",
            "Dependency Tree Visualization:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"92ab7aa1ea244a9192830de56ec9cf6f-0\" class=\"displacy\" width=\"650\" height=\"337.0\" direction=\"ltr\" style=\"max-width: none; height: 337.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">she</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">bad</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">idea</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">girl</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-92ab7aa1ea244a9192830de56ec9cf6f-0-0\" stroke-width=\"2px\" d=\"M62,202.0 62,185.33333333333334 141.0,185.33333333333334 141.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-92ab7aa1ea244a9192830de56ec9cf6f-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M62,204.0 L58,196.0 66,196.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-92ab7aa1ea244a9192830de56ec9cf6f-0-1\" stroke-width=\"2px\" d=\"M262,202.0 262,152.0 547.0,152.0 547.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-92ab7aa1ea244a9192830de56ec9cf6f-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M262,204.0 L258,196.0 266,196.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-92ab7aa1ea244a9192830de56ec9cf6f-0-2\" stroke-width=\"2px\" d=\"M362,202.0 362,168.66666666666666 544.0,168.66666666666666 544.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-92ab7aa1ea244a9192830de56ec9cf6f-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M362,204.0 L358,196.0 366,196.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-92ab7aa1ea244a9192830de56ec9cf6f-0-3\" stroke-width=\"2px\" d=\"M462,202.0 462,185.33333333333334 541.0,185.33333333333334 541.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-92ab7aa1ea244a9192830de56ec9cf6f-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M462,204.0 L458,196.0 466,196.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-92ab7aa1ea244a9192830de56ec9cf6f-0-4\" stroke-width=\"2px\" d=\"M162,202.0 162,135.33333333333331 550.0,135.33333333333331 550.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-92ab7aa1ea244a9192830de56ec9cf6f-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M550.0,204.0 L554.0,196.0 546.0,196.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Building a Chunker in NLP"
      ],
      "metadata": {
        "id": "UKmOkFa358wB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install and import necessary libraries\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.chunk import RegexpParser\n",
        "# Download required resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "# Step 2: Tokenize and perform POS tagging on a sample sentence\n",
        "sentence = input(\"enter the sting\")\n",
        "tokens = word_tokenize(sentence)\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "print(\"POS Tagged Tokens:\")\n",
        "print(tagged_tokens)\n",
        "# Step 3: Define a custom chunking grammar using regular expressions\n",
        "chunk_grammar = r\"\"\"\n",
        "NP: {<DT>?<JJ>*<NN.*>} # Noun Phrase\n",
        "VP: {<VB.*><NP|PP|CLAUSE>*} # Verb Phrase\n",
        "\"\"\"\n",
        "# Step 4: Apply chunking using RegexpParser\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "chunk_tree = chunk_parser.parse(tagged_tokens)\n",
        "# Step 5: Display and visualize the extracted chunks\n",
        "print(\"\\nChunked Sentence Tree:\")\n",
        "chunk_tree.pprint()\n",
        "\n",
        "# Optional: Visualize the chunk tree in a popup window (works only in desktop environments)\n",
        "try:\n",
        "  chunk_tree.draw()\n",
        "except:\n",
        "  print(\"Tree visualization is not supported in this environment.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpT3_Onl6Aga",
        "outputId": "d83f4593-f72c-4eef-89e2-00fcbdfa3cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the stingthe quick brown fox jumbs over lazy dog\n",
            "POS Tagged Tokens:\n",
            "[('the', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumbs', 'NN'), ('over', 'IN'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
            "\n",
            "Chunked Sentence Tree:\n",
            "(S\n",
            "  (NP the/DT quick/JJ brown/NN)\n",
            "  (NP fox/NN)\n",
            "  (NP jumbs/NN)\n",
            "  over/IN\n",
            "  (NP lazy/JJ dog/NN))\n",
            "Tree visualization is not supported in this environment.\n"
          ]
        }
      ]
    }
  ]
}